{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<left>\n",
    "    <img style=\"float: left;\" src=\"module7_heading.jpeg\" width=\"800\"></img>\n",
    "</left>\n",
    "\n",
    "<br/><br/>\n",
    "<br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Meanwhile you have learned a lot about Hypothesis Testing already. Go through the following self-study material to discover more examples, deepen your knowledge and broaden your view on Hypothesis Testing. Make sure to carefully read through the following pages, the more thorough you will go through the summary, the easier you will find the assignment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Content of this module\n",
    "\n",
    "The goal of this module is to enable you to understand all the steps that are needed to plan and execute an experimental test. This involves:\n",
    "\n",
    "* stating the problem;\n",
    "* designing the data gathering process;\n",
    "* choosing and performing a statistical test that is suited for your data.\n",
    "\n",
    "Therefore, we will start by introducing the topic of hypothesis testing. We will then consider the design of experiments, and have A/B testing as a practical example that is often used in industry.\n",
    "\n",
    "While we do not explicitly mention machine learning during this module, we stress that the topic is strictly connected to ML. For example, you might want to test the results of your newly-developed ML model against something else. Or, ML might be involved in the data preparation before the test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypothesis testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Research hypotheses vs. statistical hypotheses\n",
    "\n",
    "Research hypotheses (also called phenomenal hypotheses) are speculations and conjectures that drive research. Statistical hypotheses are a way of restating the research hypotheses so that they can be addressed by statistical techniques.\n",
    "\n",
    "A classical example, dating to Laplace (1773): he was investigating whether comets belong to the Solar system or not.\n",
    "The (mutually exclusive) research hypotheses would be:\n",
    "\n",
    "1. comets do not belong to the Solar system,\n",
    "1. comets belong to the Solar system.\n",
    "\n",
    "However, it is difficult to test these hypotheses as they are. So he reworked the problem in terms of statistical hypotheses:\n",
    "\n",
    "A.  comets have orbital parameters that follow a uniform distribution,\n",
    "A.  comets have orbital parameters that do not follow a uniform distribution.\n",
    "\n",
    "Statistical hypotheses are the subject of statistical tests. (Laplace invented a test for this case).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What to do with hypotheses: the frequentist and Bayesian perspectives\n",
    "\n",
    "Once we frame a problem in terms of statistical hypotheses, we have two choices over what to do with them.\n",
    "\n",
    "The first choice is that of classical, or _frequentist_ statistics. We assume that hypotheses are either true or false; however we don't know their truth. We can apply statistical tests to try to **disprove** a hypothesis. There is a degree of uncertainty in how safely we can disprove a hypotheses, that is quantified by a number called _p-value_.\n",
    "\n",
    "The other possible choice is that of Bayesian statistics. We assume that all hypotheses may be possible, but some are more probable than others. Therefore we can attach a probability to each hypothesis. The Bayes' rule tells how we can calculate such probabilities, given pre-existing knowledge and new data.\n",
    "\n",
    "In this module, we will only focus on frequentist statistical tests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis testing outline\n",
    "\n",
    "As mentioned above, the very basic idea when testing hypothesis is to disprove a hypothesis. In an ideal setup where there are no errors or misclassifications, disproving a hypothesis would be very simple: you would just need to find a counter-example. For example, if the hypothesis were \"All dogs are heavier than cats\", you would just need to find **one** cat that is heavier than **one** dog to disprove it. Let's use this example to identify a few key steps before moving to a more interesting example.\n",
    "\n",
    "1. Build a model for what you want to disprove. This is called the **\"null hypothesis\"**, in our example it is \"All dogs are heavier than cats\".\n",
    "2. Pick **one** event that has a low probability of happening in your model.  This is the **test**.\n",
    "3. Perform or observe the actual outcome of experiment. If the event occurs, we have evidence that the null hypothesis is false. If not, the result is inconclusive.\n",
    "\n",
    "We note that we cannot demonstrate a null hypothesis; the best we can do to confirm it is saying that we have no element at hand to disprove it. We leave the door open to observing, tomorrow, a cat that's heavier than a dog.\n",
    "\n",
    "### Hypothesis resting in the real world: significance and power\n",
    "\n",
    "When the hypothesis involves measurements (which come with experimental errors), classes (which come with misclassifications) or any kind of probability distribution, disproving a hypothesis requires more work than just looking for one counter-example. For example, we could only have scales available that weight with a precision of 1 kg. What if the smallest dog is found to be the same weight of the highest cat? Or, what if the scale gets stuck at some point, and some measurements are not representative of the animal that was weighted? The idea here is that you need **a sizable number of counter-examples** before you can confidently say that the null hypothesis is (probably) false.\n",
    "\n",
    "How many counter-examples can you attribute to errors, before becoming confident in disproving the null hypothesis? And how rare should the counter-examples be? These concepts are quantified by the **significance** and **power** of a test.\n",
    "\n",
    "Let us first recap the two types or errors in statistics:\n",
    "\n",
    "* Type I error: we reject the null hypothesis, even if it is really true.\n",
    "* Type II error: we don't reject the null hypothesis, even if it is really false.\n",
    "\n",
    "In the terminology of classification, a type I error would be a false positive, and a type II error would be a false negative.\n",
    "\n",
    "We call:\n",
    "\n",
    "* significance, denoted by $\\alpha$, the probability of making a type I error,\n",
    "* power, denoted by $\\beta$, the probability of making a type II error.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rejection\n",
    "\n",
    "What do rejection and non-rejection imply? Let's assume we have run an experiment and observed an outcome. If we find counter-examples to the null hypothesis, this means that the null hypothesis is unlikely to produce the observed outcome. Is the null hypothesis false?  Actually, it is false only if the model is accurate.\n",
    "\n",
    "How to interpret rejection depends on the source of the model. For example, a model could be:\n",
    "\n",
    "* based on expert judgment: \"an expert thinks that the null hypothesis is unlikely to produce that outcome\";\n",
    "* based on past experience: \"previous cases show that the null hypothesis is unlikely to produce that outcome\";\n",
    "* based on mathematical principles: \"under this mathematical model, the null hypothesis is unlikely to produce that outcome\".\n",
    "\n",
    "To show how easy it is to draw an incorrect conclusion because of a bad model, let's consider an example. We are asked, does the color of a vehicle influence its fuel consumption? We get data for a blue vehicle and a red vehicle, and we find a striking difference. Can we conclude that color influences fuel consumption? Not so fast: what vehicles are we measuring, exactly? What if ask for more data, and find that the blue vehicle is a small car, and the red one is a big truck?  This example is of course exaggerated, but it highlights the importance of having a good model as the foundation of our hypotheses.\n",
    "\n",
    "Let's now make a different example. We have run a survey to understand what opinion our customers have about our company. 30% of our customers answer our survey. Our expectation (null hypothesis) was that most people hold opinion A, but we actually find that opinion B is more widespread. Can we reject the null hypothesis? Consider the following figure:\n",
    "<img src=\"as_good_as_model.png\" />\n",
    "where the shaded region shows the part of the population that responded to our survey. Now it is obvious that, before rejecting the null hypothesis, we need to account for the bias in who answers the survey: maybe people with opinion A are satisfied and see little incentive in answering, while people with opinion B are critical and want their voice to be heard.\n",
    "\n",
    "Some kind of pitfalls that could arise:\n",
    "\n",
    "* Assume that things are independent when they are not, and “accidentally“ just disprove independence instead of your nominal hypothesis;\n",
    "* Assume Gaussian distributions and “accidentally” just disprove Gaussianity;\n",
    "* Assume you sample representative data, when your sample is actually not representative.\n",
    "\n",
    "Therefore, before concluding that something else than the null hypothesis must be true, one should check what exactly the null hypothesis entails: all the underlying assumptions, all the details of the model. Going back to the trucks example, if we reject the (null) hypothesis that a vehicle is red, can we conclude that therefore it must be blue? Making such a conclusion is only possible if no other colors exist.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building and choosing models\n",
    "\n",
    "Here we make a few examples of things that one could test:\n",
    "\n",
    "* Test unknown sizes again thresholds, or against other sizes. For example, test \"Is fuel consumption lower than X?\" or \"Is object type A more probable than another?\". Possible tests for this case are [binomial](https://en.wikipedia.org/wiki/Binomial_test), [z-test](https://en.wikipedia.org/wiki/Z-test) or t-test (see below).\n",
    "* Test independence between discrete things. For example, \"Is vehicle quality independent of paint, for vehicles produced this way?\" A possible test for this case is the [χ² test](https://en.wikipedia.org/wiki/Chi-squared_test).\n",
    "* Test non-existence of trends. For example, \"Does production quality remain the same over time?” A possible test for this case is the [Mann-Kendall test](https://vsp.pnnl.gov/help/vsample/design_trend_mann_kendall.htm).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An example of statistical tests\n",
    "\n",
    "Let $T$ be the test statistic. We use the test statistic to form a decision rule. We define a rejection region $R$ such that we make a decision based on if we are in or not in the rejection region, i.e.\n",
    "\n",
    "* $X \\in R$ means we reject the null hypothesis\n",
    "* $X \\not \\in R$ means we retain (do not reject) the null hypothesis\n",
    "\n",
    "usually the rejection region is simply the region where the value of the statistic is greater than some threshold, i.e.\n",
    "\n",
    "$$\n",
    "    R = \\left \\{ x : T(x) > c\\right \\}\n",
    "$$\n",
    "\n",
    "The value $c$ is called the *critical value*. The problem in hypothesis testing is to find a test-statistic and an appropriate critical value. For example, if we apply a $t$-test, then the critical value is just a quantile of the Student $t$-distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Independent two-sample t-test with unequal sample sizes, equal variance\n",
    "\n",
    "Given two groups (1, 2), this test is only applicable when it can be assumed that the two distributions have the same variance.\n",
    "\n",
    "The $t$ statistic to test whether the means are different can be calculated as follows:\n",
    "\n",
    "$${\\displaystyle t={\\frac {{\\bar {X}}_{1}-{\\bar {X}}_{2}}{s_{p}\\cdot {\\sqrt {{\\frac {1}{n_{1}}}+{\\frac {1}{n_{2}}}}}}}}$$\n",
    "\n",
    "where\n",
    "\n",
    "$${\\displaystyle s_p^2={\\frac {\\left(n_{1}-1\\right)s_{X_{1}}^{2}+\\left(n_{2}-1\\right)s_{X_{2}}^{2}}{n_{1}+n_{2}-2}}}.$$\n",
    "\n",
    "Here $s_p^2$ is the _pooled_ variance (i.e., the weighted average of variances of the two groups), and $s^2_{X_1}$ and $s^2_{X_2}$ are the [unbiased estimators](https://en.wikipedia.org/wiki/Bias_of_an_estimator) of the variances of the two samples. The denominator of $t$ is the standard error of the difference between two means. Remember that the unbiased estimator for the mean is simply $\\bar X = \\frac{1}{n} \\sum_{i=1}^n X_i$ and the unbiased estimator for the variance of a random variable $X$ is ${ s_{X}^{2}={\\frac {1}{n-1}}\\sum _{i=1}^{n}(X_{i}-{\\overline {X}}\\,)^{2}}$.\n",
    "The degrees of freedom for this test is $n_1 + n_2 − 2$ where n is the number of participants in each group.\n",
    "\n",
    "Given a set significance level, say $\\alpha=5\\%$, we use [the value of the $t$ statistic](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.t.html) in the Student $t$-distribution and reject the null hypothesis if the value is in the top $95\\%$ quantile.\n",
    "\n",
    "### Independent two-sample t-test with equal sample sizes and equal variance\n",
    "\n",
    "If the two populations have the same size, the pooling of variances becomes simpler\n",
    "\n",
    "$$\n",
    "    {\\displaystyle t={\\frac {{\\bar {X}}_{1}-{\\bar {X}}_{2}}{s_{p}{\\sqrt {\\frac {2}{n}}}}}}\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "    {\\displaystyle s_{p}={\\sqrt {\\frac {s_{X_{1}}^{2}+s_{X_{2}}^{2}}{2}}}}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confidence intervals when performing hypothesis testing\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Warning**: When we reject the null hypothesis we often say that the result is *statistically significant*. A result might be statistically significant and yet the size of the effect might be small. In such a case we have a result that is statistically significant but not practically significant.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this warning in mind, in addition to the hypothesis test explained above, it is often useful to consider the confidence interval of the difference in effect. To describe the method, let us again use the $t$-test as our base case.\n",
    "\n",
    "Let us denote $\\widehat {\\text{se}} = s_p \\sqrt{\\frac{2}{n}}$ as the standard error and $\\widehat \\theta = \\bar X_1 - \\bar X_2$, then the confidence interval for the difference of the two groups is\n",
    "\n",
    "$$\n",
    "    (\\widehat \\theta - \\widehat {\\text{se}} \\, t_{\\alpha/2}, \\widehat \\theta + \\widehat {\\text{se}}\\,  t_{\\alpha/2}).\n",
    "$$\n",
    "\n",
    "We will reject the null hypothesis that the two groups have the same mean if the confidence interval does not contain $0$. The confidence interval gives us an interval for the difference between the groups which can be very useful in decision making, as decisions usually depend on the size of differences due to costs involved in e.g. making a change to a product."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Design of experiments\n",
    "\n",
    "Design of experiments (DOE) is defined as a branch of applied statistics that deals with planning, conducting, analyzing, and interpreting controlled tests to evaluate the factors that control the value of a parameter or group of parameters. DOE is a powerful data collection and analysis tool that can be used in a variety of experimental situations, when more than one input factor is suspected to influence the output. Thus, a well-performed experiment may help to understand the key factors of a process, the settings, that lead to acceptable performance or that bring less variation to the output, and other details of a process. \n",
    "\n",
    "Many of the current statistical approaches to designed experiments originate from the work of R. A. Fisher in the early part of the 20th century. Fisher demonstrated how taking the time to seriously consider the design and execution of an experiment before trying it helped avoid frequently encountered problems in analysis. \n",
    "\n",
    "In this section we introduce some key concepts in DOE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Factors and confounders\n",
    "\n",
    "Factors, or inputs of the process, are variables that can influence the result and that can be properly tested. In machine learning, they would be called features.\n",
    "\n",
    "Confounders (or co-factors) are certain aspects that also influence the result of an experiment, but they cannot be controlled and sometimes cannot even be measured. It is important to identify them in order to understand the limitations of the experiment.\n",
    "\n",
    "When performing a test it is important to think about confounders, i.e. hidden variables that affect the outcome in such a way that reaching a conclusion can be hard. An example of a confounder can be a piece of information, presented in a newspaper, that affects a customer churn and might shaddow the effect of a test of a churn rate between two discount program. Ignoring such confounders can lead to a wrong conclusions.\n",
    "\n",
    "Another example of a confounder, that is commonly encountered with customers, is a word-of-mouth and social media, where, for example, customers might learn that they are participating in an experiment, and this can have devastating impact on the analysis. We will talk about blindness of an experiment below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choice of subjects\n",
    "\n",
    "When picking the sample that you wish to split in two parts, test and control, for obtaining valid and reliable results, you need to make sure that the group as a whole is representative of the group that you later will use the conclusion on. \n",
    "\n",
    "There are several factors to consider. First of all, the size of the study group: the more subjects you include, the greater the statistical power is, determining the confidence you can have in the results. However, how you assign subjects to groups plays also a crucial role. \n",
    "\n",
    "Assume we have a test group, i.e. a group of subjects or individuals, we are conducting the experiment on. Note that for the purpose of this section we consider experiments with only one test group, however there are cases of experiment design with several study groups. You should also include a control group, that represents the subjects without any experimental intervention.\n",
    "\n",
    "One of the examples is a churn mitigation in a customer base. Imagine the company wants to target those who are likely to churn. Using some algorithm to target them, they choose a group of customers that are likely to churn and do an AB test (for details on the test see the relevant section of the reading material) with some discount program. They notice that there is such a large effect of the discount program that they roll it out to all customers. However it shows that on the whole customer base (not just on the likely-to-churn customers) the discount program does not have an expected effect on the churn and simply it is hemorrhaging money."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomization\n",
    "\n",
    "Comparing two groups that are not statistically equivalent can lead to biased results. This is the entire reason we do randomized splits. Since the split is entirely random it will make the groups statistically equivalent and hence comparable. To make an example of non comparability, look at the effect of an ad about toys where the test group have kids and the control group does not. This is a very extreme example but it highlights the problem well.\n",
    "\n",
    "\n",
    "### Blocking and stratification\n",
    "\n",
    "An experiment can be completely randomized, when every subject is assigned to a group completely at random, or randomized using randomized block design. At the later case subjects are first systematized according to some characteristic they share and then randomly assigned to test and control groups within these assemblage. For example, subjects can be first grouped by age, and then randomly distributed to test and control groups. \n",
    "\n",
    "A very similar notion is that of \"stratified design\". Stratified sampling is terminology from sampling design, and blocking is a similar concept from experimental design. In each case they represent a restriction on randomization in an attempt to reduce nuisance variation. You could call it stratification, if you try to spread a confounder over all the sample; or blocking, if you can find groups (blocks) of subjects where the value of the confounder is the same, and then you randomly assign the treatments inside the block."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replication\n",
    "\n",
    "The experiment should be set up and documented so that replication can happen at two levels: data should be gathered and kept so that the analysis can be repeated; documentation and tools should be such that the data gathering can be repeated. Replication of an experiment allows to estimate the variability of the results and also to increase the accuracy of the estimate, assuming that no systematic bias is present in the experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Blindness\n",
    "\n",
    "If the test subjects are aware of what group of the test they are in, they might have biased behavior. For example, you do not want people to know they are getting a placebo instead of a drug since it will lose its effect.\n",
    "\n",
    "However, the people who administrate the test, for instance the doctors, might also treat the subject in a different way, even unconsciously, depending on the group they belong to. Therefore, they should also not know what arm of the experiment the subjects belong to. This is called a double-blind test. The information about the assignments of the subjects to the groups should be kept only by a third party, and only released at the end of the experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test significance and effect size\n",
    "\n",
    "A key rule is that _p-values_ should be decided before the experiment is run. However, very strong _p-values_ can be impossible to achieve if the test size is not large enough.\n",
    "\n",
    "When conducting an experiment it is important to decide the effect size that you wish to detect. Setting the significance level and desired effect size, one can compute the minimum number of individuals needed to reach these goals. A usual mistake is not accounting beforehand for the desired effect size, which often results to the test groups not big enough, and thus to overall a useless experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A/B testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A/B testing is a form of testing one (or more) hypotheses in an experiment. It starts with making a research hypothesis, then it includes stating the problem in terms of statistical hypotheses, designing an experiment to gather the relevant data, and finally applying hypothesis testing.\n",
    "\n",
    "The name \"A/B testing\" is due to the random split of a population into two groups (A and B). Another name of A/B testing is Randomized Controlled Trial (RCT). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An example\n",
    "\n",
    "A/B testing is used to determine which alternative is best. Let's say that we are working with a drug and wish to test if it is more effective in treating a disease than the currently used drug. To test this we simply give the new drug to one group and the old drug to another group. We then count what percentage were successfully treated in each group. The question now becomes: since there is so much variation between different people, how can we be certain that if we observe a difference, for instance the new drug might have been more successful, the difference is not due to random variations? This is the problem that A/B testing aims to solve. As soon as there is some randomness involved, like the drug working with some probability, then we can never be certain. What we can make sure is that we have a decision rule which allows us to make a certain percentage of mistakes, but be wary as the impact of the number of mistakes made, depends heavily on the circumstance.\n",
    "\n",
    "But let us not digress, here are some examples of A/B testing\n",
    "\n",
    "* Amazon initially decided to launch their first personalized product recommendations based on an A/B test showing a huge revenue increase by adding that feature.\n",
    "* LinkedIn tested whether to use the top slot on a user's stream for top news articles or an encouragement to add more contacts.\n",
    "* Spotify tested all changes concerning the UI on their platform.\n",
    "\n",
    "The important fact is that there are two groups that you can compare, i.e. a test group (that you test your change on) and a control group (which is the baseline). Here the design of the experiment is very important, for instance, if there is a `burn in` effect. Take for example a product to which you make a radical change, and wish to test if it is better than the previous product. In this case, the initial response to the product might not be the same as the long term effect (like novelty or anger towards change). On the other hand, if you are testing a small change or if there is otherwise no risk of `burn in`, then A/B testing can be really useful.\n",
    "\n",
    "When designing an A/B test it is important that you choose a good metric, i.e. a measure that you really care about. If other parts of your company are doing A/B testing, make sure that you have agreed on what is the most important metric. Perhaps you are doing an analysis on customers and one analyst chooses the metric `revenue` and the other chooses `retention`. These can in many cases be quite different metrics and as such your conclusions will potentially work against each other.\n",
    "\n",
    "To make sure that there is no systematic difference between the test and control group, we do a so called Randomized Controlled Trial (RCT). We will discuss this next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RCT (Randomized Controlled Trial)\n",
    "The basic layout of a randomized controlled trial is built from the following parts:\n",
    "\n",
    "* The first step is to determine who is eligible to receive treatment and exclude those who cannot be treated.\n",
    "* The next step is to choose a sample size and do two random subsamples of the same size, where one subsample is the *intent to treat* (ITT) group and the other the non-ITT group, sometimes called the control group.\n",
    "* Some of the individuals in the ITT group will refuse or become unavailable for treatment due to some factors that we cannot control, these will be the not-treated and the remaining are the treated group."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"RCT.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the trial is done we usually want to estimate the effect of the treatment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin with the hypothesis\n",
    "\n",
    "```h_0: The two groups have the same mean```\n",
    "\n",
    "```h_1: The two groups do not have the same mean```\n",
    "\n",
    "The basics of A/B testing is that we want to test if the **null hypothesis** `h_0` holds or not with some certainty. Since they are normally distributed we can simply apply the $t$-test. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example A/B, change of probability\n",
    "\n",
    "We have seen how to test the difference between a mean value between two groups, but a frequently occurring testing problem is when we want to check if there is a change in probability or \"rate\" between a test and control group. Here we will look at the problem of \"conversion rate\" with ads.\n",
    "\n",
    "Let's assume that we have a product on sale, and a certain percentage of the customers buy the product. The percentage is an estimate of the probability that a randomly chosen customer will buy the product, call it $p$. The question we would like to answer is, if a certain ad for the product changes the probability of buying the product. So the question is about the value of $p$ in the two groups, where the random variable is [Bernoulli](https://en.wikipedia.org/wiki/Bernoulli_distribution) distributed.\n",
    "\n",
    "Specifically, let us assume that we have two groups, one with estimated probability $\\hat p_1$ and $n_1$ samples, the other with estimated probability $\\hat p_2$ and $n_2$ samples. We can think of these observations coming from the test and control groups respectively. In this case the exact test is the so called [Fisher's exact test](https://en.wikipedia.org/wiki/Fisher%27s_exact_test). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fisher's exact test is computationally expensive, but it might be good to know that it exists if we have a small number of samples. However, usually we have a fairly large amount of data and thus we can use a normal approximation instead. To perform the normal approximation test, we can simply use the $\\hat p_1$ and $\\hat p_2$ as the mean of the two populations with standard deviation $s_{X_1}^2 = \\hat p_1 (1-\\hat p_1)$, and $s_{X_2}^2 = \\hat p_2 (1-\\hat p_2)$. The test statistic becomes\n",
    "\n",
    "$${\\displaystyle t={\\frac {{\\bar {X}}_{1}-{\\bar {X}}_{2}}{s_{p}\\cdot {\\sqrt {{\\frac {1}{n_{1}}}+{\\frac {1}{n_{2}}}}}}}}$$\n",
    "\n",
    "where\n",
    "\n",
    "$${\\displaystyle s_{p}={\\sqrt {\\frac {\\left(n_{1}-1\\right)s_{X_{1}}^{2}+\\left(n_{2}-1\\right)s_{X_{2}}^{2}}{n_{1}+n_{2}-2}}}.}$$\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "**Normal Approximation**: The normal approximation comes from the fact that the [sample mean](https://en.wikipedia.org/wiki/Sample_mean_and_covariance) is approximately normally distributed when we have large samples. How many samples are needed depend on the distribution from which we sample, but in general the sample sizes are big enough that we can use the normal approximation. If you are unsure there is tons of information on Google about this specific subject. For the more theoretically interested it is due to the [law of large numbers](https://en.wikipedia.org/wiki/Law_of_large_numbers) and the [central limit theorem](https://en.wikipedia.org/wiki/Central_limit_theorem). In the case of Bernoulli random variables we can use the rule of thumb that $np \\geq 5$ and $n(1-p) \\geq 5$, but take this with a grain of salt.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example of change of probability continued\n",
    "\n",
    "So, let's get back to our example with ads and change of conversion rate. To put things in numbers, say that we do a test with a group of $400$ customers that we randomly split into two groups, call them test and control respectively. We then show the test-group a new ad and the control-group an old ad. The numbers come in and we get $100$ purchases out of $200$ customers in the test group and $70$ purchases out of $200$ customers in the control group.\n",
    "\n",
    "In this case we get $\\hat p_1 = 0.5$, $s_{X_1}^2 = \\hat p_1 (1-\\hat p_1) = 0.25$ and $\\hat p_2 = 0.35$, $s_{X_2}^2 = 0.35*(1-0.35) = 0.2275$. The test values becomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0698670605799054"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "p1 = 0.5\n",
    "p2 = 0.35\n",
    "s1 = np.sqrt(0.25)\n",
    "s2 = np.sqrt(0.2275)\n",
    "n1 = 200\n",
    "n2 = 200\n",
    "sp = np.sqrt(((200-1)*s1**2 + (200-1)*s2**2)/(200+200-2))\n",
    "t = (p1-p2)/(sp*np.sqrt(1/200+1/200))\n",
    "t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us assume that we beforehand decided that the significance level was $5\\%$ and let us look the the corresponding quantile of the t-distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.9659423239761926"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scipy.stats as stats\n",
    "alpha = 0.05\n",
    "c = stats.t.ppf(df=n1+n2-2,q=1-alpha/2)\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method ppf in module scipy.stats._distn_infrastructure:\n",
      "\n",
      "ppf(q, *args, **kwds) method of scipy.stats._continuous_distns.t_gen instance\n",
      "    Percent point function (inverse of `cdf`) at q of the given RV.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    q : array_like\n",
      "        lower tail probability\n",
      "    arg1, arg2, arg3,... : array_like\n",
      "        The shape parameter(s) for the distribution (see docstring of the\n",
      "        instance object for more information)\n",
      "    loc : array_like, optional\n",
      "        location parameter (default=0)\n",
      "    scale : array_like, optional\n",
      "        scale parameter (default=1)\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    x : array_like\n",
      "        quantile corresponding to the lower tail probability q.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Just some info about the stats.t.sf function above\n",
    "help(stats.t.ppf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our test statistic was $t \\approx 3.07$ and it is greater than our critical value $c \\approx 1.97$, therefore we should reject our null hypothesis that the probabilities are the same. But as  we talked about earlier, often it more illuminating to work with confidence intervals. So let us use the method mentioned above and compute the confidence interval. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "se = sp*np.sqrt(1/200+1/200) # Standard error\n",
    "hattheta = p1-p2 # estimate of the difference\n",
    "left_edge = hattheta - c*se  # The lower boundary of the confidence interval\n",
    "right_edge = hattheta + c*se # The upper boundary of the confidence interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p1-p2 = 0.150\n",
      "Confidence interval for p1-p2 = (0.054,0.246)\n"
     ]
    }
   ],
   "source": [
    "print(\"p1-p2 = %.3f\" % hattheta)\n",
    "print(\"Confidence interval for p1-p2 = (%.3f,%.3f)\" % (left_edge, right_edge))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we come to the same conclusion, but this time we know that the difference can range from fairly small to very large in favor of the new ad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other approaches and further reading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One problem of A/B testing is that you need to set aside a group that you will not test on. Furthermore you need to gather enough data before you make your decision, see `early peeking`. There is a different approach to A/B testing called the multi-armed bandit experiment. The setup here is more along the lines of online decisions. For instance, you might have a stream of users coming to a website and once we see that one alternative is clearly better, we want to add more users to that alternative right away. News websites often do this with their headlines.\n",
    "\n",
    "The most well known method for multi-armed bandits is the so called Thompson sampling method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showURL(url, ht=500):\n",
    "    \"\"\"Return an IFrame of the url to show in notebook with height ht\"\"\"\n",
    "    from IPython.display import IFrame\n",
    "    return IFrame(url, width='95%', height=ht) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"95%\"\n",
       "            height=\"400\"\n",
       "            src=\"https://en.wikipedia.org/wiki/Thompson_sampling\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fc645004c70>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "showURL('https://en.wikipedia.org/wiki/Thompson_sampling',400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A nice tutorial of how to implement this, along with some theory, can be found in the following blog-post: [Towards Data Science](https://towardsdatascience.com/beyond-a-b-testing-multi-armed-bandit-experiments-1493f709f804). If you want to read more about multi-armed bandits, see [https://en.wikipedia.org/wiki/Multi-armed_bandit](https://en.wikipedia.org/wiki/Multi-armed_bandit)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<center>Combient Mix AB All Rights Reserved</center>\n",
    "\n",
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
